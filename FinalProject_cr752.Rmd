---
title: "Article Analysis in New York Times Articles and Data Analysis for the PhD"
author: "Chunyue Ren"
date: "5/6/2019"
output:
  pdf_document: default

  
---

### GitHubLink: https://github.com/ChunyueRen/DataWrangling

# 1.Introduction

This project is aimed to do some analysis for the articles in the New York Times using the API applied from the website.Also, I will analysis the data from a website which will provide some information about the admission of doctoral students.

I used the packages as below:
"rvest", "curl", "tidyverse", "dplyr", "stringr", "repurrrsive", "wordcloud", "igraph"


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE,warning = FALSE)
#install.packages("rvest")
#install.packages("curl")
#install.packages("tidyverse")
#install.packages("dplyr")
#install.packages("stringr")
#install.packages("repurrrsive")
#install.packages("wordcloud")
#install.packages("igraph")
#install.packages("rmarkdown")
library(tibble)
library(tidyverse)
library(rvest)
library(lubridate)
library(repurrrsive)
library(rvest)
library(httr)
library(tidytext)
library(curl)
library(jsonlite)
library(dplyr)
library(stringr)
library(wordcloud)
library(dplyr)
library(stringr)
library(igraph)

```


# 2.Clean the data which is the articles extraced from New York Times and do some analysis

First, I need to apply for a API from NewYork Times API website. Then I will get the useful information I want. Then I check the node of the website for the part I want to extract. Then I found that the node is ".evys1bk0". So I use them to extract the articles I want. After looking at the data, I found that some of the essay can not be extracted since we have to pay for it if I want to read it. So what I need to do is to delete the useless parts.
Finally, I write the articles which I have cleaned into the file named ArticleTidyData.csv.




```{r clean the data,echo = FALSE,warning=FALSE}
#Prepare the data, clean the data
source("api-keys.R")
url <- paste0(
  "https://api.nytimes.com/svc/mostpopular/v2/viewed/1.json?api-key=",
  api.key.nytimes
)
url_article <- url %>% fromJSON() %>% as.data.frame() %>% select(website = results.url)

# extract the context of the top 20 essays
# using some tools, I found the right node of the context we want, which is called ".evys1bk0"
for(i in 1:nrow(url_article)){
a <- url_article[i,1] %>% read_html() %>% html_nodes(".evys1bk0") %>% html_text()
  b <- a[1]
  
for(j in 2:length(a)){
  b <- paste(b,a[j], sep = "") 
}
url_article$test[i] = b
}
#after looking at the data, I found that some of the essay can not be extracted since we have to pay for it if I want to read it. So what I need to do is to delete the useless parts.
url_article_tidy <- url_article[-which(url_article$test=="NANANA"),]
# Then I have the tidy data into a file. Then I can use that file to analyse
write.csv(url_article_tidy,".//ArticleTidyData.csv",row.names = FALSE)
```

## 2.1 Get the word frequences with the barplot

For this part, I used the packages named "tidyverse" and "stringr" to make get the frequences of the word. First, I need to delete the irrelevant words such as stop words. Then I counted the word of them and make the result as part of the table. After that, I used the ggplot to make the result can be seen directly.




```{r word frequence,warning=FALSE,echo = FALSE}
data_tidy <- as.data.frame(read.csv(".//ArticleTidyData.csv"))
#data_tidy_1 <- data_tidy[,-which(colnames(data_tidy) == "website")]
#(1)doing the word frequence analysis

url_article_count <- url_article_tidy %>% 
  unnest_tokens(word, test) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  filter(n>10)%>%
  mutate(word = reorder(word, n))
ggplot(url_article_count, aes(word, y=n))+
  geom_bar(stat = "identity",fill = "steelblue")+
  theme_minimal()+
  coord_flip()
```

With the barplot, you can clearly see the order of word frequency.For example, the top three words are "ms","people" and "trump" which are make sense since "trump" is popular word.

## 2.2 Word cloud for the articles


Then, I tried another method to make the result can be seen interestingly. I used the package named "wordcloud". With this package, I can get the wordcloud of the result I got in last section. I add some parameters to control the picture I want. I set the min.freq = 1,max.words=70, random.order=FALSE, rot.per=0.35, and colors=brewer.pal(8, "Dark2"). 

We can get the word cloud below.

```{r word cloud,warning=FALSE,echo = FALSE}

#Use wordcloud to make the results look more intuitive
set.seed(123)
wordcloud(words = url_article_count$word, freq = url_article_count$n, min.freq = 1,
          max.words=70, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))



```

Also, I use another way to present this result and make the results look more interesting.That's the word cloud. We can easily see that the popular words are “ms”, “people” and “trump” just at a glance.  



## 2.3 Sentiment analysis for the articles

In the previous sections, I explored in depth what we mean by the tidy text format and showed how this format can be used to approach questions about word frequency. This allowed us to analyze which words are used most frequently in documents and to compare documents, but now let’s investigate a different topic. Let’s address the topic of opinion mining or sentiment analysis. When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust. We can use the tools of text mining to approach the emotional content of text programmatically.

There are a variety of methods and dictionaries that exist for evaluating the opinion or emotion in text. The tidytext package contains several sentiment lexicons in the sentiments dataset.

Here I use the sentiments "bing" to analyze the sentiments for all the articles. I can be seen below.(16 articles in total) 



```{r Sentiment of the articles,echo = FALSE, warning=FALSE,error = FALSE }
library(janeaustenr)
library(dplyr)
library(stringr)

a <- url_article_tidy %>%
  mutate(linenumber = row_number()) %>%
  ungroup()
c <- a %>% unnest_tokens(word, test) %>%
  group_by(linenumber) %>%
  count(word, sort = TRUE) %>% 
  inner_join(get_sentiments("bing")) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
c_1 <- c %>% filter(linenumber < 5)
ggplot(c_1, aes(word, sentiment, fill = linenumber)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~linenumber, ncol = 2, scales = "free_x")

c_2 <- c %>% filter((linenumber<9) & (linenumber > 4))
ggplot(c_2, aes(word, sentiment, fill = linenumber)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~linenumber, ncol = 2, scales = "free_x")

c_3 <- c %>% filter((linenumber<13) & (linenumber > 8))
ggplot(c_3, aes(word, sentiment, fill = linenumber)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~linenumber, ncol = 2, scales = "free_x")

c_4 <- c %>% filter((linenumber<17) & (linenumber > 12))
ggplot(c_4, aes(word, sentiment, fill = linenumber)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~linenumber, ncol = 2, scales = "free_x")

#c_5 <- c %>% filter((linenumber<21) & (linenumber > 16))
#ggplot(c_5, aes(word, sentiment, fill = linenumber)) +
#  geom_col(show.legend = FALSE) +
# facet_wrap(~linenumber, ncol = 2, scales = "free_x")


```


From these pictures for each article, we can know the setiment trend of the whole article. Also,we can make comparison between them.
For example, the article 1 and article 2 are neutral articles. But for the article 3, there are a very positive part for that article.
The Emotional ups and downs of the article 5, article 7, article 9 and article 10 are intense.Other articles are relatively emotionally calm.



## 2.4 Correlation analysis for the articles

Correlation is a statistical method used to assess a possible linear association between two continuous variables. It is simple both to calculate and to interpret. 

When we want to know more about several articles, correlation analysis would be a better choice.

At first, I establish matrices for these articles by word, taking some of them for further analysis.


```{r correlation analysis for article, echo = FALSE,warning=FALSE}
#Take part of the article for correlation analysis
main <- a %>% unnest_tokens(word, test) %>%
  anti_join(stop_words) %>%
  group_by(linenumber) %>%
  count(word, sort = TRUE)
main_matrix <- table(unique(main[1:50, c("word", "linenumber")]))
#head(main_matrix)
main_matrix_adjacency <- main_matrix %*% t(main_matrix)
#head(main_matrix_adjacency)
```

From the last step, we have made full preparation for the following analysis. I can use the correlation matrix to get the Cluster Dendrogram for these articles.

```{r hclust for article, echo = FALSE,warning=FALSE}
main_norm <- main_matrix / rowSums(main_matrix)

main_hclust <- hclust(dist(main_norm, method = "manhattan"))

plot(main_hclust)
```


From the picture, you can clearly see the relationship between them.

## 2.5 Using heat map to show the relationship 


A heat map (or heatmap) is a graphical representation of data where the individual values contained in a matrix are represented as colors. "Heat map" is a newer term but shading matrices have existed for over a century.

Here I use the package named "viridisLite" to get the heatmap, also set some parameters such as color with the code like "col = viridis(n = 256, alpha = 1, begin = 0, end = 1, option = "A")"

```{r matrix_adjacency analysis with heatmap,echo = FALSE,warning=FALSE}
install.packages("viridisLite")
library(viridis) # for colors
heatmap(main_matrix_adjacency, col = viridis(n = 256, alpha = 1, begin = 0, end = 1, option = "A"))
```

From the heat map, we can see the relationship between these articles more clearly.

## 2.6 Timeline for the articles


Geom_path() connects the observations in the order in which they appear in the data. I use this method to make other analysis for the data I have to find more latent relationship between them.


```{r Timeline analysis for these articles,echo = FALSE,warning=FALSE}
main_ordering <- main_hclust$labels[main_hclust$order]

main_plot <- main %>%
    filter(n() > 1) %>%        
    ungroup() %>%
    mutate(order_essay = as.numeric(factor(linenumber)),
          word = factor(word, levels = main_ordering))

ggplot(main_plot, aes(order_essay, word)) +
    geom_point() +
    geom_path(aes(group = order_essay))
```


Also, the relationship can be shown in this way. We can see the word similarities with the picture shown. We can see that "people", "trump" and "house" are the high frequency vocabulary. Also, we can conclude that “record” and "campaign" is lower frequency vocabulary. 

## 2.7 some vertices-and-edges 

As we have had a matrix_adjacency, we can also establish some vertices and edges to see the relationship between these articles. With the format of graph, we can see other interesting thing between these articles I extracted from the website.



```{r graph_adjacency for the articles,echo = FALSE,warning=FALSE}
main_graph <- graph.adjacency(main_matrix_adjacency, 
      weighted = TRUE, mode = "undirected", diag = FALSE)
main_graph2 <- delete_vertices(main_graph,
      V(main_graph)[diag(main_matrix_adjacency) < 10])

plot(main_graph, edge.width = E(main_graph)$weight/5, 
     layout = layout_with_fr)


```


Finally, I can find the relationship in this way. Add some vertices-and-edges to find some latent relationship between these articles. We can see from the picture that some words like "air", "women" and so on are seperated in the picture. On the other hand, the word "community", "house" and so on gather together. Then we can know what kind of articles have close links. 


# 3.Analysis for the PHD data


I get the data from the website to read the file. I found there are a lot of interesting things can be analyzed from the data. The data have four parameters. There is broad_field, such as "Life sciences", "Mathematics and computer sciences", "Psychology and social sciences", "Engineering", "Education", "Humanities and arts" and "Other". For the column of "Major_field", there are "Agricultural sciences and natural resources", "Biological and biomedical sciences", "Health sciences", "Chemistry", "Geosciences, atmospheric sciences, and ocean sciences" and "Physics and astronomy". There are still a lot of other parameters, which I will show them in the following analysis.





```{r setupfor2, echo = FALSE,warning=FALSE}
#analyze the PHD data prepare the data 
#install.packages("dplyr")
library(dplyr)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(magrittr)
library(stringr)
library(ggplot2)
library(data.table)

phd <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv")
```

## 3.1 boxplots of $R^{2}$ by major

$R^{2}$ is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. Whereas correlation explains the strength of the relationship between an independent and dependent variable, R-squared explains to what extent the variance of one variable explains the variance of the second variable. So, if the $R^{2}$ of a model is 0.50, then approximately half of the observed variation can be explained by the model's inputs.

Here I split the data by major_field and use map() to calculate by major_field, the $R^{2}$ for the linear model n_phds ~ year + field. Then I use ggplot2 making  a set of boxplots of $R^{2}$ by broad_field. 



```{r boxplot for the data from PHD,echo = FALSE}
#processing the analysis 
r_square <- phd %>%
  split(.$major_field) %>%
  map(~(lm(n_phds ~ year + field, data = .))) %>%
  map(summary) %>%
  map_dbl(~.$r.squared)

major_field <- names(r_square)
rsquare <- rep(0,length(r_square))
for(i in 1:length(r_square)){
    rsquare[i] <- r_square[i] 
}

box <- cbind.data.frame(major_field, rsquare)

plot <- phd %>%
  select(major_field,broad_field) %>%
  group_by(major_field,broad_field) %>%
  right_join(box, by = c("major_field" = "major_field")) %>%
  unique() %>%
  arrange(broad_field)

ggplot(plot, aes(x = broad_field, y = rsquare)) + geom_boxplot()
```


You can find directly that the major of Psychology and social science has the highest R-square. However,the major of Education has the lowest R-square. With this figure, we can get a conclusion easily.

## 3.2 The trend of number of PhDs awarded 

Next, I want to know more about hte PhDs awarded. For each field, I rank each year by the number of  PhDs awarded. I use facet_wrap() to draw different graphs for different field.


```{r rank1 analysis for the data from PHD,echo = FALSE}

rank1 <- phd %>%
  group_by(field) %>%
  arrange(field, year) %>%
  mutate(award_rank1 = min_rank(desc(n_phds)))
# make the result can be seen more directly, just extract some of them to see the trend of the number of phd in different field.

ggplot(rank1[1:50,], aes(year, award_rank1, colour = n_phds)) +
  geom_point() + 
  geom_line(aes(group=1)) +
  facet_wrap(~ field, scales = "free_x")
```


The number for each major's award is changed year by year. The average is almost 5. From the graph of Accounting, we can see that the number of award is decreasing until 2014. After 2014, the number of award of Acounting is increasing. From the graph of Accoustics, we can know that the number of award is changing these years. Finally, the number of award of Accoustics keep increasing from 2013. For the field of continuing teacher education, the number of award is increasing until 2014. After that, there is a little drop, and increase again. For the field of African History, the number of award is almost decreasing until 2011. After that, the number of award keeps increasing. For the last field, we can see that the number of award is changing until 2012. After that year, the number award is almost decreasing. 



## 3.3 Total award for majors

For this part, I want to know which field has the most PhD award. Then I use group_by() to group the data into different major_field. Then, I use ggplot to get for the total award they get for each major_field.

 

```{r rank2 analysis for the data from PHD,echo = FALSE}
rank2 <- phd %>%
  group_by(major_field, field) %>%
  summarise(total_award = sum(n_phds,na.rm = TRUE)) %>%
  arrange(major_field, field) %>%
  mutate(award_rank2 = min_rank(desc(total_award)))

# make the result can be seen more directly, just extract some of them to see the variance of the number of award_rank2 in different field.
ggplot(rank2[1:50,], aes(field, award_rank2, colour = total_award)) +
  geom_point() + 
  geom_line(aes(group=1)) +
  facet_wrap(~ major_field, scales = "free_x")
```

From the plot, we can see the change for each major in the same field.Through this picture, we can have a holistic understanding of the Ph.D. awards for each department.

## 3.4 Number of PhDs awarded for fields

For this part, I want to know which broad_field has the most PhD award. Then I use group_by() to group the data into different broad_field. Then, I use ggplot to get for the total award they get for each broad_field.



```{r rank3 analysis for the data from PHD,echo = FALSE}
rank3 <- phd %>%
  group_by(broad_field, year) %>% 
  summarise(total_award1 = sum(n_phds, na.rm = TRUE)) %>%
  arrange(broad_field, year) %>%
  mutate(award_rank3 = min_rank(desc(total_award1)))

ggplot(rank3[1:50,], aes(year, award_rank3, colour = total_award1)) +
  geom_point() + 
  geom_line(aes(group=1)) +
  facet_wrap(~ broad_field, scales = "free_x")
```




The number for each field's award is changed year by year. Most of them are decreasing. For the "Education", we can see that the number of award is increasing until 2011. Then there is a drop after that year. For the "Engineering", we can see that the number of award is almost decreasing these years. For the "Humanities and Arts", the number of award is decreasing until 2012. Then there is a little increase after 2012. For the "Life science", the number of award is decreasing all these year. For the "Mathematics and Computer Science", the numebr of award is nearly decreasing all these year.







```{r result for rank1 ,echo = FALSE}
#rank1
```

```{r result for rank2,echo = FALSE}
##rank2
```

```{r result for rank3,echo = FALSE}
#rank3
```


## 3.5 More PhDs than last year

For this part, I want to find every year with more PhDs awarded than the previous year.I used the lag() to get the result.



```{r lag calculate,echo = FALSE}
lag1 <- phd %>%
  group_by(field) %>%
  filter( n_phds > lag(n_phds))
lag1
```


List all the fields which can have more PhDs awarded than the previous year every year. We can get the information from the tibble easily.

## 3.6 Quartiles for number of PhDs awarded by major

For this part, I want to know the quartiles for number of PhDs awarded by broad_field.Then, I can get the tibble below.

```{r quatile calculate,echo = FALSE}
quatile_phd <- phd %>% 
  group_by(broad_field) %>%
  summarise(sum=sum(n_phds,na.rm = T)) %>% group_by(quartile = ntile(sum,4))

quatile_phd

```

From the tibble, we can know each quartiles for number of PhDs awarded by broad_field.

## 3.7 Function to judge

For this part, I want to write a function that inputs a character string for broad_field and returns the signal-to-noise ratio aggregated over field and year. Also, write the function so that an error message is displayed if the character input doesnot belong tobroad_field.


```{r function for PHD,echo = FALSE}
# write a function to judge whether the major is belong to the Phd or not
judge <- function(x) {
  phd_function <- fread("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv")
  if ( x %in% (unique(phd_function$broad_field)) ) {
   a1 <-  phd_function[broad_field == x]
   signal = mean(a1$n_phds, na.rm = T)/sd(a1$n_phds, na.rm = T)
   print(signal)
  } 
  else {
   print("It is not belong to the phd table")
  }
}
judge("Education")
```


Here I write a function named "judge" to figure out whether the input belongs to the data. If it belongs to the data, we will get the signal-to-noise ratio.

I enter "Education", then I get the signal-to-noise ratio for the Education major is 0.7600295.



# 4.Conclusion

From this project, I cleaned the data, analyzed the implicit relationships that exist within the data, and presented them using a variety of graphics. Also, I used a lot of packages and functions learned in the course, which do help me a lot for this final project.


