---
title: "Article Analysis in New York Times Articles and Data Analysis for the PhD"
author: "Chunyue Ren"
date: "5/6/2019"
output:
  pdf_document: default
  word_document: default
---
# 1.Introduction

This project is aimed to do some analysis for the articles in the New York Times using the API applied from the website.Also, I will analysis the data from a website which will provide some information about the admission of doctoral students


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE,warning = FALSE)
#install.packages("rvest")
#install.packages("curl")
#install.packages("tidyverse")
#install.packages("dplyr")
#install.packages("stringr")
#install.packages("repurrrsive")
#install.packages("wordcloud")
#install.packages("igraph")
#install.packages("rmarkdown")
library(tibble)
library(tidyverse)
library(rvest)
library(lubridate)
library(repurrrsive)
library(rvest)
library(httr)
library(tidytext)
library(curl)
library(jsonlite)
library(dplyr)
library(stringr)
library(wordcloud)
library(dplyr)
library(stringr)
library(igraph)

```


# 2.Clean the data which is the articles extraced from New York Times and do some analysis


```{r clean the data,echo = FALSE,warning=FALSE}
#Prepare the data, clean the data
source("api-keys.R")
url <- paste0(
  "https://api.nytimes.com/svc/mostpopular/v2/viewed/1.json?api-key=",
  api.key.nytimes
)
url_article <- url %>% fromJSON() %>% as.data.frame() %>% select(website = results.url)

# extract the context of the top 20 essays
# using some tools, I found the right node of the context we want, which is called ".evys1bk0"
for(i in 1:nrow(url_article)){
a <- url_article[i,1] %>% read_html() %>% html_nodes(".evys1bk0") %>% html_text()
  b <- a[1]
  
for(j in 2:length(a)){
  b <- paste(b,a[j], sep = "") 
}
url_article$test[i] = b
}
#after looking at the data, I found that some of the essay can not be extracted since we have to pay for it if I want to read it. So what I need to do is to delete the useless parts.
url_article_tidy <- url_article[-which(url_article$test=="NANANA"),]
# Then I have the tidy data into a file. Then I can use that file to analyse
write.csv(url_article_tidy,".//ArticleTidyData.csv",row.names = FALSE)
```

## 2.1 Get the word frequences with the barplot

```{r word frequence,warning=FALSE,echo = FALSE}
data_tidy <- as.data.frame(read.csv(".//ArticleTidyData.csv"))
#data_tidy_1 <- data_tidy[,-which(colnames(data_tidy) == "website")]
#(1)doing the word frequence analysis

url_article_count <- url_article_tidy %>% 
  unnest_tokens(word, test) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  filter(n>10)%>%
  mutate(word = reorder(word, n))
ggplot(url_article_count, aes(word, y=n))+
  geom_bar(stat = "identity",fill = "steelblue")+
  theme_minimal()+
  coord_flip()
```

With the barplot, you can clearly see the order of word frequency.For example, the top three words are "ms","people" and "trump" which are make sense since "trump" is popular word.

## 2.2 Word cloud for the articles

```{r word cloud,warning=FALSE,echo = FALSE}

#Use wordcloud to make the results look more intuitive
set.seed(123)
wordcloud(words = url_article_count$word, freq = url_article_count$n, min.freq = 1,
          max.words=70, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))



```

Also, I use another way to present this result and make the results look more interesting.That's the word cloud. We can easily see that the popular words are “ms”, “people” and “trump” just at a glance.  



## 2.3 Setiment analysis for the articles



```{r Sentiment of the articles,echo = FALSE, warning=FALSE,error = FALSE }
library(janeaustenr)
library(dplyr)
library(stringr)

a <- url_article_tidy %>%
  mutate(linenumber = row_number()) %>%
  ungroup()
c <- a %>% unnest_tokens(word, test) %>%
  group_by(linenumber) %>%
  count(word, sort = TRUE) %>% 
  inner_join(get_sentiments("bing")) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
c_1 <- c %>% filter(linenumber < 5)
ggplot(c_1, aes(word, sentiment, fill = linenumber)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~linenumber, ncol = 2, scales = "free_x")

c_2 <- c %>% filter((linenumber<9) & (linenumber > 4))
ggplot(c_2, aes(word, sentiment, fill = linenumber)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~linenumber, ncol = 2, scales = "free_x")

c_3 <- c %>% filter((linenumber<13) & (linenumber > 8))
ggplot(c_3, aes(word, sentiment, fill = linenumber)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~linenumber, ncol = 2, scales = "free_x")

c_4 <- c %>% filter((linenumber<17) & (linenumber > 12))
ggplot(c_4, aes(word, sentiment, fill = linenumber)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~linenumber, ncol = 2, scales = "free_x")

#c_5 <- c %>% filter((linenumber<21) & (linenumber > 16))
#ggplot(c_5, aes(word, sentiment, fill = linenumber)) +
#  geom_col(show.legend = FALSE) +
# facet_wrap(~linenumber, ncol = 2, scales = "free_x")


```


From these pictures for each article, we can know the setiment trend of the whole article. Also,we can make comparison between them.
For example, the article 1 and article 2 are neutral articles. But for the article 3, there are a very positive part for that article.
The Emotional ups and downs of the article 5, article 7, article 9 and article 10 are intense.Other articles are relatively emotionally calm

## 2.4 Correlation analysis for the articles

At first, I establish matrices for these articles. 


```{r correlation analysis for article, echo = FALSE,warning=FALSE}
#Take part of the article for correlation analysis
main <- a %>% unnest_tokens(word, test) %>%
  anti_join(stop_words) %>%
  group_by(linenumber) %>%
  count(word, sort = TRUE)
main_matrix <- table(unique(main[1:50, c("word", "linenumber")]))
#head(main_matrix)
main_matrix_adjacency <- main_matrix %*% t(main_matrix)
#head(main_matrix_adjacency)
```

Then get the Cluster Dendrogram for the article.

```{r hclust for article, echo = FALSE,warning=FALSE}
main_norm <- main_matrix / rowSums(main_matrix)

main_hclust <- hclust(dist(main_norm, method = "manhattan"))

plot(main_hclust)
```


From the picture, you can clearly see the relationship between them.

## 2.5 Using heat map to show the relationship 

```{r matrix_adjacency analysis with heatmap,echo = FALSE,warning=FALSE}
install.packages("viridisLite")
library(viridis) # for colors
heatmap(main_matrix_adjacency, col = viridis(n = 256, alpha = 1, begin = 0, end = 1, option = "A"))
```

From the heat map, we can see the relationship between these articles more clearly.

## 2.6 Timeline for the articles

```{r Timeline analysis for these articles,echo = FALSE,warning=FALSE}
main_ordering <- main_hclust$labels[main_hclust$order]

main_plot <- main %>%
    filter(n() > 1) %>%        # scenes with > 1 character
    ungroup() %>%
    mutate(order_essay = as.numeric(factor(linenumber)),
          word = factor(word, levels = main_ordering))

ggplot(main_plot, aes(order_essay, word)) +
    geom_point() +
    geom_path(aes(group = order_essay))
```

Also, the relationship can be shown in this way.

## 2.7 some vertices-and-edges 

```{r graph_adjacency for the articles,echo = FALSE,warning=FALSE}
main_graph <- graph.adjacency(main_matrix_adjacency, 
      weighted = TRUE, mode = "undirected", diag = FALSE)
main_graph2 <- delete_vertices(main_graph,
      V(main_graph)[diag(main_matrix_adjacency) < 10])

plot(main_graph, edge.width = E(main_graph)$weight/5, 
     layout = layout_with_fr)


```

Finally, I can find the relationship in this way. Add some vertices-and-edges to find some latent relationship between these articles.

# 3.Analysis for the PHD data

```{r setupfor2, echo = FALSE,warning=FALSE}
#analyze the PHD data prepare the data 
#install.packages("dplyr")
library(dplyr)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(magrittr)
library(stringr)
library(ggplot2)
library(data.table)

phd <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv")
```

## 3.1 boxplots of $R^{2}$ by major

```{r boxplot for the data from PHD,echo = FALSE}
#processing the analysis 
r_square <- phd %>%
  split(.$major_field) %>%
  map(~(lm(n_phds ~ year + field, data = .))) %>%
  map(summary) %>%
  map_dbl(~.$r.squared)

major_field <- names(r_square)
rsquare <- rep(0,length(r_square))
for(i in 1:length(r_square)){
    rsquare[i] <- r_square[i] 
}

box <- cbind.data.frame(major_field, rsquare)

plot <- phd %>%
  select(major_field,broad_field) %>%
  group_by(major_field,broad_field) %>%
  right_join(box, by = c("major_field" = "major_field")) %>%
  unique() %>%
  arrange(broad_field)

ggplot(plot, aes(x = broad_field, y = rsquare)) + geom_boxplot()
```



You can find directly that the major of Psychology and social science has the highest R-square. However,the major of Education has the lowest R-square. 

## 3.2 The trend of number of PhDs awarded 


```{r rank1 analysis for the data from PHD,echo = FALSE}

rank1 <- phd %>%
  group_by(field) %>%
  arrange(field, year) %>%
  mutate(award_rank1 = min_rank(desc(n_phds)))
# make the result can be seen more directly, just extract some of them to see the trend of the number of phd in different field.

ggplot(rank1[1:50,], aes(year, award_rank1, colour = n_phds)) +
  geom_point() + 
  geom_line(aes(group=1)) +
  facet_wrap(~ field, scales = "free_x")
```


The number for each major's award is changed year by year. The average is almost 5.

## 3.3 Total award for majors

```{r rank2 analysis for the data from PHD,echo = FALSE}
rank2 <- phd %>%
  group_by(major_field, field) %>%
  summarise(total_award = sum(n_phds,na.rm = TRUE)) %>%
  arrange(major_field, field) %>%
  mutate(award_rank2 = min_rank(desc(total_award)))

# make the result can be seen more directly, just extract some of them to see the variance of the number of award_rank2 in different field.
ggplot(rank2[1:50,], aes(field, award_rank2, colour = total_award)) +
  geom_point() + 
  geom_line(aes(group=1)) +
  facet_wrap(~ major_field, scales = "free_x")
```

From the plot, we can see the change for each major in the same field.

## 3.4 Number of PhDs awarded for fields

```{r rank3 analysis for the data from PHD,echo = FALSE}
rank3 <- phd %>%
  group_by(broad_field, year) %>% 
  summarise(total_award1 = sum(n_phds, na.rm = TRUE)) %>%
  arrange(broad_field, year) %>%
  mutate(award_rank3 = min_rank(desc(total_award1)))

ggplot(rank3[1:50,], aes(year, award_rank3, colour = total_award1)) +
  geom_point() + 
  geom_line(aes(group=1)) +
  facet_wrap(~ broad_field, scales = "free_x")
```

The number for each field's award is changed year by year. Most of them are decreasing.

```{r result for rank1 ,echo = FALSE}
#rank1
```

```{r result for rank2,echo = FALSE}
##rank2
```

```{r result for rank3,echo = FALSE}
#rank3
```


## 3.5 More PhDs than last year


```{r lag calculate,echo = FALSE}
lag1 <- phd %>%
  group_by(field) %>%
  filter( n_phds > lag(n_phds))
lag1
```


List all the fields which can have more PhDs awarded than the previous year every year. We can get the information from the tibble easily.

## 3.6 Quartiles for number of PhDs awarded by major


```{r quatile calculate,echo = FALSE}
quatile_phd <- phd %>% 
  group_by(broad_field) %>%
  summarise(sum=sum(n_phds,na.rm = T)) %>% group_by(quartile = ntile(sum,4))

quatile_phd

```

From the tibble, we can know each quartiles for number of PhDs awarded by broad_field.

## 3.7 Function to judge


```{r function for PHD,echo = FALSE}
# write a function to judge whether the major is belong to the Phd or not
judge <- function(x) {
  phd_function <- fread("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv")
  if ( x %in% (unique(phd_function$broad_field)) ) {
   a1 <-  phd_function[broad_field == x]
   signal = mean(a1$n_phds, na.rm = T)/sd(a1$n_phds, na.rm = T)
   print(signal)
  } 
  else {
   print("It is not belong to the phd table")
  }
}
judge("Education")
```


Here I write a function named "judge" to figure out whether the input belongs to the data. If it belongs to the data, we will get the signal-to-noise ratio.
I enter "Education", then I get the signal-to-noise ratio for the Education major is 0.7600295.



# 4.Conclusion

From this project, I cleaned the data, analyzed the implicit relationships that exist within the data, and presented them using a variety of graphics. Also, I used a lot of packages and functions learned in the course, which do help me a lot for this final project.


